{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM,GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14422796308512904380\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 357302272\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 12625082302388410089\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset used in this project to train and evaluate the pipeline is from [WMT](http://www.statmt.org/). I am only using a small vocabulary of the original corpus as of now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split('\\n')\n",
    "\n",
    "english_sentences = load_data('data/small_vocab_en')\n",
    "french_sentences = load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the sentences, it looks like they have been preprocessed already.  The puncuations have been delimited using spaces. All the text have been converted to lowercase.  But, the text still requires more preprocessing.\n",
    "### Vocabulary\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Here, I'm having a look at the complexity of the dataset I'm working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "### Tokenize (IMPLEMENTATION)\n",
    "Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "# Testing Tokenizer\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding (IMPLEMENTATION)\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "This is done by using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    if not length:\n",
    "        length = max([len(sentence) for sentence in x])\n",
    "    return pad_sequences(x, maxlen=length, padding='post')\n",
    "\n",
    "# Testing Padding\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "These are the various neural network architecturesyou I'm experimenting with in this project.\n",
    "- Model 1 is a simple vanilla RNN\n",
    "- Model 2 is a RNN with Embedding\n",
    "- Model 3 is a Bidirectional RNN\n",
    "- Model 4 is an Encoder-Decoder RNN\n",
    "- Model 5 is a Bidirectional RNN with Embedding\n",
    "\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids.  As, we want the French translation, the function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Vanilla RNN\n",
    "A basic RNN model is a good baseline for sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 12s 109us/step - loss: 2.7112 - acc: 0.4608 - val_loss: nan - val_acc: 0.5316\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 10s 91us/step - loss: 1.7717 - acc: 0.5678 - val_loss: nan - val_acc: 0.5933\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 10s 91us/step - loss: 1.5147 - acc: 0.6023 - val_loss: nan - val_acc: 0.6106\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 10s 91us/step - loss: 1.3909 - acc: 0.6199 - val_loss: nan - val_acc: 0.6298\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 10s 91us/step - loss: 1.3019 - acc: 0.6347 - val_loss: nan - val_acc: 0.6408\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 10s 91us/step - loss: 1.2284 - acc: 0.6468 - val_loss: nan - val_acc: 0.6548\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 10s 91us/step - loss: 1.1698 - acc: 0.6563 - val_loss: nan - val_acc: 0.6601\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 10s 91us/step - loss: 1.1214 - acc: 0.6647 - val_loss: nan - val_acc: 0.6653\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 10s 91us/step - loss: 1.0848 - acc: 0.6691 - val_loss: nan - val_acc: 0.6711\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 10s 91us/step - loss: 1.0548 - acc: 0.6738 - val_loss: nan - val_acc: 0.6758\n",
      "new jersey est parfois calme en l' de il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    input_sequence = Input(shape=(input_shape[1:]))\n",
    "    rnn = GRU(english_vocab_size, return_sequences=True)(input_sequence)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
    "    predictions = Activation('softmax')(logits)\n",
    "    model = Model(inputs=input_sequence, outputs=predictions)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding\n",
    "An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors. This model creates a RNN model using embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 12s 112us/step - loss: 3.2397 - acc: 0.4178 - val_loss: 2.5230 - val_acc: 0.4822\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 12s 107us/step - loss: 2.0147 - acc: 0.5575 - val_loss: 1.5297 - val_acc: 0.6386\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 12s 107us/step - loss: 1.2880 - acc: 0.6902 - val_loss: 1.0872 - val_acc: 0.7403\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 12s 107us/step - loss: 0.9311 - acc: 0.7682 - val_loss: 0.7898 - val_acc: 0.7919\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 12s 107us/step - loss: 0.6953 - acc: 0.8114 - val_loss: 0.6154 - val_acc: 0.8283\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 12s 107us/step - loss: 0.5654 - acc: 0.8382 - val_loss: 0.5180 - val_acc: 0.8494\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 12s 107us/step - loss: 0.4805 - acc: 0.8589 - val_loss: 0.4484 - val_acc: 0.8671\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 12s 108us/step - loss: 0.4186 - acc: 0.8753 - val_loss: 0.4043 - val_acc: 0.8770\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 12s 108us/step - loss: 0.3760 - acc: 0.8858 - val_loss: 0.3543 - val_acc: 0.8919\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 12s 107us/step - loss: 0.3407 - acc: 0.8955 - val_loss: 0.3369 - val_acc: 0.8963\n",
      "new jersey est parfois calme en cours de l' automne et il est <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    input_sequence = Input(shape=(input_shape[1],))\n",
    "    embedding_sequence = Embedding(\n",
    "        input_dim=english_vocab_size,\n",
    "        output_dim=128,\n",
    "        input_length=input_shape[1:][0])(input_sequence)\n",
    "    rnn = GRU(200, return_sequences=True)(embedding_sequence)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size))(rnn)\n",
    "    predictions = Activation('softmax')(logits)\n",
    "    \n",
    "    model = Model(inputs=input_sequence, outputs=predictions)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size + 1,\n",
    "    french_vocab_size + 1)\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bidirectional RNNs\n",
    "The restriction of a RNN is that it can't see the future input, only the past.  This is where bidirectional recurrent neural networks come in.  They are able to see the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 17s 150us/step - loss: 2.2795 - acc: 0.5364 - val_loss: nan - val_acc: 0.6058\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 16s 143us/step - loss: 1.4122 - acc: 0.6195 - val_loss: nan - val_acc: 0.6325\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 16s 143us/step - loss: 1.2524 - acc: 0.6445 - val_loss: nan - val_acc: 0.6548\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 16s 143us/step - loss: 1.1586 - acc: 0.6647 - val_loss: nan - val_acc: 0.6745\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 16s 143us/step - loss: 1.0864 - acc: 0.6793 - val_loss: nan - val_acc: 0.6824\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 16s 143us/step - loss: 1.0294 - acc: 0.6883 - val_loss: nan - val_acc: 0.6910\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 16s 143us/step - loss: 0.9859 - acc: 0.6949 - val_loss: nan - val_acc: 0.6970\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 16s 143us/step - loss: 0.9488 - acc: 0.7009 - val_loss: nan - val_acc: 0.7044\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 16s 143us/step - loss: 0.9166 - acc: 0.7062 - val_loss: nan - val_acc: 0.7068\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 16s 143us/step - loss: 0.8889 - acc: 0.7102 - val_loss: nan - val_acc: 0.7113\n",
      "new jersey est parfois calme en mois et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    input_sequence = Input(input_shape[1:])\n",
    "    rnn = Bidirectional(GRU(english_vocab_size, return_sequences=True))(input_sequence)\n",
    "    logits = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(rnn)\n",
    "    model = Model(inputs=input_sequence, outputs=logits)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "bd_rnn_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "bd_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(bd_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Encoder-Decoder\n",
    "This model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence.  The decoder takes this matrix as input and predicts the translation as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 22s 200us/step - loss: 2.9418 - acc: 0.4421 - val_loss: nan - val_acc: 0.4837\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 21s 192us/step - loss: 2.2290 - acc: 0.5128 - val_loss: nan - val_acc: 0.5320\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 21s 192us/step - loss: 1.9061 - acc: 0.5444 - val_loss: nan - val_acc: 0.5615\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 21s 192us/step - loss: 1.6598 - acc: 0.5686 - val_loss: nan - val_acc: 0.5732\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 21s 192us/step - loss: 1.5520 - acc: 0.5849 - val_loss: nan - val_acc: 0.5943\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 21s 192us/step - loss: 1.4683 - acc: 0.6015 - val_loss: nan - val_acc: 0.6095\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 21s 193us/step - loss: 1.4090 - acc: 0.6113 - val_loss: nan - val_acc: 0.6184\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 21s 193us/step - loss: 1.3626 - acc: 0.6199 - val_loss: nan - val_acc: 0.6179\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 21s 193us/step - loss: 1.3312 - acc: 0.6251 - val_loss: nan - val_acc: 0.6284\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 21s 193us/step - loss: 1.3064 - acc: 0.6308 - val_loss: nan - val_acc: 0.6369\n",
      "new jersey est parfois agréable en mois et il est est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    input_sequence = Input(input_shape[1:])\n",
    "    encoder = LSTM(english_vocab_size)(input_sequence)\n",
    "    decoder = RepeatVector(output_sequence_length)(encoder)\n",
    "    decoder = LSTM(english_vocab_size, return_sequences=True)(decoder)\n",
    "    decoded = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(decoder)\n",
    "\n",
    "    model = Model(inputs=input_sequence, outputs=decoded)\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "    \n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "\n",
    "encdec_rnn_model = encdec_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "encdec_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(encdec_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Bidirectional RNN with Embedding\n",
    "This model incorporates embedding and a bidirectional rnn into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 38s 348us/step - loss: 2.4003 - acc: 0.4863 - val_loss: nan - val_acc: 0.6019\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 37s 333us/step - loss: 1.2174 - acc: 0.6656 - val_loss: nan - val_acc: 0.7118\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 37s 334us/step - loss: 0.8592 - acc: 0.7500 - val_loss: nan - val_acc: 0.7831\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 37s 332us/step - loss: 0.6304 - acc: 0.8114 - val_loss: nan - val_acc: 0.8365\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 37s 333us/step - loss: 0.4465 - acc: 0.8663 - val_loss: nan - val_acc: 0.8851\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 37s 334us/step - loss: 0.2976 - acc: 0.9148 - val_loss: nan - val_acc: 0.9297\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 37s 333us/step - loss: 0.2023 - acc: 0.9432 - val_loss: nan - val_acc: 0.9499\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 37s 332us/step - loss: 0.1803 - acc: 0.9487 - val_loss: nan - val_acc: 0.9276\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 37s 331us/step - loss: 0.1374 - acc: 0.9606 - val_loss: nan - val_acc: 0.9651\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 37s 332us/step - loss: 0.1056 - acc: 0.9695 - val_loss: nan - val_acc: 0.9683\n",
      "new jersey est parfois calme pendant l' automne il il est neigeux en neigeux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    input_sequence = Input(shape=(input_shape[1:]))\n",
    "    embed = Embedding(english_vocab_size, 128, input_length=input_shape[1:][0])(input_sequence)\n",
    "    \n",
    "    encoder = Bidirectional(GRU(256))(embed)\n",
    "    dense_layer = Dense(128, activation='relu')(encoder)\n",
    "    decoding_layer = RepeatVector(output_sequence_length)(dense_layer)\n",
    "    \n",
    "    decoder = Bidirectional(GRU(256, return_sequences=True))(decoding_layer)\n",
    "    preds = TimeDistributed(Dense(french_vocab_size,activation='softmax'))(decoder)\n",
    "    \n",
    "    model = Model(inputs=input_sequence, outputs=preds)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(lr=0.005),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "\n",
    "model_rnn_final = model_final(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "\n",
    "model_rnn_final.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "print(logits_to_text(model_rnn_final.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/20\n",
      "110288/110288 [==============================] - 34s 309us/step - loss: 2.2582 - acc: 0.5038 - val_loss: nan - val_acc: 0.6211\n",
      "Epoch 2/20\n",
      "110288/110288 [==============================] - 32s 294us/step - loss: 1.1572 - acc: 0.6780 - val_loss: nan - val_acc: 0.7128\n",
      "Epoch 3/20\n",
      "110288/110288 [==============================] - 32s 295us/step - loss: 0.8378 - acc: 0.7493 - val_loss: nan - val_acc: 0.7848\n",
      "Epoch 4/20\n",
      "110288/110288 [==============================] - 32s 294us/step - loss: 0.5861 - acc: 0.8194 - val_loss: nan - val_acc: 0.8579\n",
      "Epoch 5/20\n",
      "110288/110288 [==============================] - 32s 294us/step - loss: 0.3880 - acc: 0.8854 - val_loss: nan - val_acc: 0.9086\n",
      "Epoch 6/20\n",
      "110288/110288 [==============================] - 32s 292us/step - loss: 0.2444 - acc: 0.9319 - val_loss: nan - val_acc: 0.9453\n",
      "Epoch 7/20\n",
      "110288/110288 [==============================] - 32s 292us/step - loss: 0.1703 - acc: 0.9525 - val_loss: nan - val_acc: 0.9510\n",
      "Epoch 8/20\n",
      "110288/110288 [==============================] - 32s 292us/step - loss: 0.1338 - acc: 0.9619 - val_loss: nan - val_acc: 0.9634\n",
      "Epoch 9/20\n",
      "110288/110288 [==============================] - 32s 292us/step - loss: 0.1145 - acc: 0.9672 - val_loss: nan - val_acc: 0.9642\n",
      "Epoch 10/20\n",
      "110288/110288 [==============================] - 32s 292us/step - loss: 0.1235 - acc: 0.9643 - val_loss: nan - val_acc: 0.9665\n",
      "Epoch 11/20\n",
      "110288/110288 [==============================] - 32s 292us/step - loss: 0.0963 - acc: 0.9721 - val_loss: nan - val_acc: 0.9659\n",
      "Epoch 12/20\n",
      "110288/110288 [==============================] - 32s 292us/step - loss: 0.0798 - acc: 0.9769 - val_loss: nan - val_acc: 0.9727\n",
      "Epoch 13/20\n",
      "110288/110288 [==============================] - 32s 292us/step - loss: 0.0696 - acc: 0.9799 - val_loss: nan - val_acc: 0.9748\n",
      "Epoch 14/20\n",
      "110288/110288 [==============================] - 32s 292us/step - loss: 0.0667 - acc: 0.9807 - val_loss: nan - val_acc: 0.9736\n",
      "Epoch 15/20\n",
      "110288/110288 [==============================] - 32s 292us/step - loss: 0.0670 - acc: 0.9804 - val_loss: nan - val_acc: 0.9751\n",
      "Epoch 16/20\n",
      "110288/110288 [==============================] - 32s 293us/step - loss: 0.0888 - acc: 0.9740 - val_loss: nan - val_acc: 0.9669\n",
      "Epoch 17/20\n",
      "110288/110288 [==============================] - 32s 293us/step - loss: 0.0652 - acc: 0.9810 - val_loss: nan - val_acc: 0.9768\n",
      "Epoch 18/20\n",
      "110288/110288 [==============================] - 32s 293us/step - loss: 0.0692 - acc: 0.9802 - val_loss: nan - val_acc: 0.9773\n",
      "Epoch 19/20\n",
      "110288/110288 [==============================] - 32s 293us/step - loss: 0.0576 - acc: 0.9833 - val_loss: nan - val_acc: 0.9786\n",
      "Epoch 20/20\n",
      "110288/110288 [==============================] - 32s 293us/step - loss: 0.0495 - acc: 0.9857 - val_loss: nan - val_acc: 0.9805\n",
      "Sample 1:\n",
      "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    model = model_final(input_shape = x.shape,\n",
    "                        output_sequence_length = y.shape[1],\n",
    "                        english_vocab_size = len(x_tk.word_index),\n",
    "                        french_vocab_size = len(y_tk.word_index))\n",
    "\n",
    "    model.fit(x, y, batch_size=1024, epochs=20, validation_split=0.2)\n",
    "\n",
    "    \n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def translate_eng_sentence(eng_sen, french_sen=''): \n",
    "    loaded_model = load_model(\"final_model\")\n",
    "    sentence = eng_sen\n",
    "    sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=preproc_english_sentences.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], preproc_english_sentences[0]])\n",
    "    predictions = loaded_model.predict(sentences, len(sentences))\n",
    "    y_id_to_word = {value: key for key, value in french_tokenizer.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "    print('English Sentence    : he saw a old yellow truck')\n",
    "    print('Model Prediction    :', ' '.join([y_id_to_word[np.argmax(preproc_english_sentences)] for preproc_english_sentences in predictions[0]]).replace('<PAD>',''))\n",
    "#     print('Expected Translation: Il a vu un vieux camion jaune')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
